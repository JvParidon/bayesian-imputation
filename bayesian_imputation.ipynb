{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do with Missing Not At Random observations: using PyMC3's Bound variables to perform Bayesian imputation of censored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib backend to display plots inline\n",
    "%matplotlib notebook\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "# resize plots to fit labels inside bounding box\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "# MPI color scheme\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions for statistical modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize cf. Gelman recommendation (to get scale comparable to unstandardized binary variables)\n",
    "def standardize(df):\n",
    "    return (df - df.mean()) / (2 * df.std())\n",
    "\n",
    "# specify and run model\n",
    "def forestplot(model, transform=np.array, **kwargs):\n",
    "    pm.forestplot(model.backend.trace,\n",
    "                  varnames=model.fixed_terms.keys(),\n",
    "                  transform=transform,\n",
    "                  **kwargs)\n",
    "    g = plt.gca()\n",
    "    g.set(xlim=(None, None))\n",
    "    no_effect = float(transform(0))\n",
    "    g.axes.axvline(no_effect, color='red')\n",
    "    g.axes.annotate('no effect', [no_effect, 0], rotation=90, va='top', ha='right', color='red')\n",
    "    \n",
    "# get posterior mode\n",
    "def posterior_mode(trace):\n",
    "    def _posterior_mode(samples, bins=500):\n",
    "        samples = samples[np.isfinite(samples)]\n",
    "        xmin = np.min(samples)\n",
    "        xmax = np.max(samples)\n",
    "        kernel = scipy.stats.gaussian_kde(samples)\n",
    "        density = kernel.pdf(np.linspace(xmin, xmax, bins))\n",
    "        step = (xmax - xmin) / bins\n",
    "        return xmin + np.argmax(density) * step\n",
    "    return np.apply_along_axis(_posterior_mode, 0, trace)\n",
    "\n",
    "# add posterior mode to summary by default\n",
    "def summary(trace, **kwargs):\n",
    "    return pm.summary(trace,\n",
    "                      extend=True,\n",
    "                      stat_funcs=[lambda x: pd.Series(posterior_mode(x), name='mode')],\n",
    "                      **kwargs)\n",
    "\n",
    "# compare models\n",
    "def compare(models, **kwargs):\n",
    "    for model in models:\n",
    "        model.backend.model.name = ' + '.join(model.terms.keys())\n",
    "    models = {model.backend.model: model.backend.trace for model in models}\n",
    "    comparison = pm.compare(models, **kwargs)\n",
    "    pm.compareplot(comparison)\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating some data\n",
    "For this simple demonstration, we'll use two normally distributed predictors and a normally distributed error term to construct our observed variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomnormal = np.random.normal(0, 5, (1000, 3))\n",
    "df_sim = pd.DataFrame(randomnormal, columns=['x1', 'x2', 'e'])\n",
    "df_sim['y'] = 30 + df_sim['x1'] + (df_sim['x2'] * 2) + df_sim['e']\n",
    "g = sns.distplot(df_sim['y'])\n",
    "g.set(title='y observed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Censoring the data\n",
    " Just imagine we're in a place where the average temperature is a nice warm 30 degrees, with some normally distributed variability. We have a thermometer that tops out at 40 degrees, so we'll cut our observed variable off at 40. That means occasionally our thermometer will indicate 40, but we know that it's quite likely to be warmer. We just don't know how much warmer, exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_right = 40\n",
    "df_sim_obs = df_sim.loc[df_sim['y'] < cutoff_right]\n",
    "g = sns.distplot(df_sim_obs['y'])\n",
    "g.set(title='y observed without censored observations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_cens = df_sim.loc[df_sim['y'] > cutoff_right]\n",
    "g = sns.distplot(df_sim_cens['y'])\n",
    "g.set(title='y observed censored observations only');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling y observed without the censored observations\n",
    "We're setting deliberately weak priors; it doesn't matter much here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sim_model:\n",
    "    \n",
    "    intercept = pm.Normal('Intercept', mu=0, sd=1.0)\n",
    "    x1 = pm.Normal('beta_x1', mu=0, sd=1.0)\n",
    "    x2 = pm.Normal('beta_x2', mu=0, sd=1.0)\n",
    "    \n",
    "    mu = intercept + (x1 * df_sim_obs['x1']) + (x2 * df_sim_obs['x2'])\n",
    "    sigma = pm.HalfNormal('y_sd', sd=1.0)\n",
    "\n",
    "    observed = pm.Normal('y', mu=mu, sd=sigma, observed=df_sim_obs['y'])\n",
    "\n",
    "    print('Named variables being sampled:')\n",
    "    [print(f'\\t{key}') for key in sorted(list(sim_model.named_vars.keys()))]\n",
    "    sim_model_trace = pm.sample(5000, tune=1000, init='advi', n_init=50000, chains=2, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pm.traceplot(sim_model_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(summary(sim_model_trace).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the intercept and the coefficients for both x1 and x2 are being underestimated.  \n",
    "The values we were expecting are 30, 1.0, and 2.0, respectively, and the posterior modes are all about 10% too low.\n",
    "We're also underestimating the variance of our observed values: the standard deviation of y is actually 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling y observed while imputing censored observations\n",
    "We'll sample the censored observations from a separate distribution that is itself a parameter of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sim_model_imputed:\n",
    "    \n",
    "    intercept = pm.Normal('Intercept', mu=0, sd=1.0)\n",
    "    x1 = pm.Normal('x1', mu=0, sd=1.0)\n",
    "    x2 = pm.Normal('x2', mu=0, sd=1.0)\n",
    "    \n",
    "    mu_cens = intercept + (x1 * df_sim_cens['x1']) + (x2 * df_sim_cens['x2'])\n",
    "    mu = intercept + (x1 * df_sim_obs['x1']) + (x2 * df_sim_obs['x2'])\n",
    "    sigma = pm.HalfNormal('y_sd', sd=1.0)\n",
    "\n",
    "    right_censored = pm.Bound(pm.Normal, lower=cutoff_right)('right_censored', mu=mu_cens, sd=sigma, shape=len(df_sim_cens))\n",
    "    observed = pm.Normal('y', mu=mu, sd=sigma, observed=df_sim_obs['y'])\n",
    "    \n",
    "    print('Named variables being sampled:')\n",
    "    [print(f'\\t{key}') for key in sorted(list(sim_model_imputed.named_vars.keys()))]\n",
    "    sim_model_imputed_trace = pm.sample(5000, tune=1000, init='advi', n_init=50000, chains=2, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " g = pm.traceplot(sim_model_imputed_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(summary(sim_model_imputed_trace).head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the coefficient estimates are much better. We're still underestimating the average temperature (Intercept) a little, but all the posterior modes are close to the parameters we used to simulate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model checks\n",
    "The imputation has produced good posteriors, but as we will now demonstrate, the unconventional sampling breaks some of the methods we would normally use to check our model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior predictive check\n",
    "ppc = pm.sample_posterior_predictive(sim_model_imputed_trace, samples=10000, model=sim_model_imputed)\n",
    "g = sns.distplot([n.mean() for n in ppc['y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks terrible, but that's because the posterior predictive distribution only includes the observations that weren't censored. The observed mean that is plotted here is inflated by the uncensored observations in the original dataset.  \n",
    "Let's include the imputed values. These can be pulled from the sampling trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot([n.mean() for n in sim_model_imputed_trace['right_censored']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the posterior predictive and the imputed y values together gives us our proper y distribution.  \n",
    "This becomes clear when we overlay the mean of the uncensored observed y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot([np.mean(np.hstack([a, b])) for a, b in zip(ppc['y'], sim_model_imputed_trace['right_censored'])])\n",
    "\n",
    "# plot uncensored data mean line\n",
    "line_x = df_sim['y'].mean()\n",
    "g.axes.axvline(line_x, color='red')\n",
    "g.axes.annotate(' observed mean', [line_x, 0], rotation=90, va='bottom', ha='right', color='red')\n",
    "g.set(title='posterior predictive distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(x=ppc['y'].mean(axis=0), y=df_sim_obs['y'])\n",
    "g.set(title='scatterplot', xlabel='predicted', ylabel='observed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot looks flattened at the top right, as we would expect because the values over 40 are censored.  \n",
    "Even better than this scatterplot would be a quantile-quantile plot, but neither seaborn nor PyMC3 have that functionality built in, so we'll just write our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# quantile-quantile plot function\n",
    "def qqplot(predicted, observed, n=101):\n",
    "    n = np.linspace(0, 100, n)\n",
    "    x = np.percentile(predicted, n)\n",
    "    y = np.percentile(observed, n)\n",
    "    g = sns.scatterplot(x, y)\n",
    "    lower = np.min([x, y])\n",
    "    upper = np.max([x, y])\n",
    "    g.axes.plot((lower, upper), (lower, upper), color='red')\n",
    "    g.set(title='quantile-quantile plot', xlabel='predicted', ylabel='observed')\n",
    "    return g\n",
    "    \n",
    "g = qqplot(ppc['y'].mean(axis=0), df_sim_obs['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put the posterior predicted y values and the imputed y values together again, we can extend the q-q plot into the censored range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = qqplot(np.hstack([ppc['y'].mean(axis=0), sim_model_imputed_trace['right_censored'].mean(axis=0)]),\n",
    "           np.hstack([df_sim_obs['y'], df_sim_cens['y']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still looks a little wonky around the censoring point (40) and just above it, but it seems to be doing okay overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
